---
title: "Spanish Pre-Trained BERT Model and Evaluation Data"
date: 2020-01-01
publishDate: 2022-04-18T22:08:23.748893Z
authors: ["José Cañete", "Gabriel Chaperon", "Rodrigo Fuentes", "Jou-Hui Ho", "Hojin Kang", "Jorge Pérez"]
tags:
- spanish
- language model
- BERT
- BETO
- evaluation
- benchmark
- dataset
publication_types: ["1"]
abstract: "The Spanish language is one of the top 5 spoken languages in the world. Nevertheless, finding resources to train or evaluate Spanish language models is not an easy task. In this paper we help bridge this gap by presenting a BERT-based language model pre-trained exclusively on Spanish data. As a second contribution, we also compiled several tasks specifically for the Spanish language in a single repository much in the spirit of the GLUE benchmark. By fine-tuning our pre-trained Spanish model we obtain better results compared to other BERT-based models pre-trained on multilingual corpora for most of the tasks, even achieving a new state-of-the-art on some of them. We have publicly released our model, the pre-training data and the compilation of the Spanish benchmarks."
featured: false
publication: "*PML4DC at ICLR 2020*"
url_pdf: https://users.dcc.uchile.cl/~jperez/papers/pml4dc2020.pdf
url_code: https://github.com/dccuchile/beto
url_slides: https://docs.google.com/presentation/d/17XHKoOTh_GwY4ZziEBH4qWJl8BkJ4P98HXxZAmUwR6g/edit#slide=id.g7fa90aae96_0_23
url_video: https://pml4dc.github.io/iclr2020/program/pml4dc_10.html
links:
- name: Models
  url: https://huggingface.co/dccuchile
---

The Spanish language is one of the top 5 spoken languages in the world. Nevertheless, finding resources to train or evaluate Spanish language models is not an easy task. In this paper we help bridge this gap by presenting a BERT-based language model pre-trained exclusively on Spanish data. As a second contribution, we also compiled several tasks specifically for the Spanish language in a single repository much in the spirit of the GLUE benchmark. By fine-tuning our pre-trained Spanish model we obtain better results compared to other BERT-based models pre-trained on multilingual corpora for most of the tasks, even achieving a new state-of-the-art on some of them. We have publicly released our model, the pre-training data and the compilation of the Spanish benchmarks.
<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Jou-Hui Ho | OpenCENIA</title><link>https://opencenia.github.io/author/jou-hui-ho/</link><atom:link href="https://opencenia.github.io/author/jou-hui-ho/index.xml" rel="self" type="application/rss+xml"/><description>Jou-Hui Ho</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 01 Jan 2020 00:00:00 +0000</lastBuildDate><image><url>https://opencenia.github.io/media/logo_huadb352de640ebaad14838e8a393df198_92131_300x300_fit_lanczos_3.png</url><title>Jou-Hui Ho</title><link>https://opencenia.github.io/author/jou-hui-ho/</link></image><item><title>Spanish Pre-Trained BERT Model and Evaluation Data</title><link>https://opencenia.github.io/models/nlp/canete-cfp-2020/</link><pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate><guid>https://opencenia.github.io/models/nlp/canete-cfp-2020/</guid><description>&lt;p>The Spanish language is one of the top 5 spoken languages in the world. Nevertheless, finding resources to train or evaluate Spanish language models is not an easy task. In this paper we help bridge this gap by presenting a BERT-based language model pre-trained exclusively on Spanish data. As a second contribution, we also compiled several tasks specifically for the Spanish language in a single repository much in the spirit of the GLUE benchmark. By fine-tuning our pre-trained Spanish model we obtain better results compared to other BERT-based models pre-trained on multilingual corpora for most of the tasks, even achieving a new state-of-the-art on some of them. We have publicly released our model, the pre-training data and the compilation of the Spanish benchmarks.&lt;/p></description></item></channel></rss>
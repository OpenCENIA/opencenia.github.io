<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><meta name=author content="OpenCENIA"><meta name=description content="In recent years there have been considerable advances in pre-trained language models, where non-English language versions have also been made available. Due to their increasing use, many lightweight versions of these models (with reduced parameters) have also been released to speed up training and inference times. However, versions of these lighter models (e.g., ALBERT, DistilBERT) for languages other than English are still scarce. In this paper we present ALBETO and DistilBETO, which are versions of ALBERT and DistilBERT pre-trained exclusively on Spanish corpora. We train several versions of ALBETO ranging from 5M to 223M parameters and one of DistilBETO with 67M parameters. We evaluate our models in the GLUES benchmark that includes various natural language understanding tasks in Spanish. The results show that our lightweight models achieve competitive results to those of BETO (Spanish-BERT) despite having fewer parameters. More specifically, our larger ALBETO model outperforms all other models on the MLDoc, PAWS-X, XNLI, MLQA, SQAC and XQuAD datasets. However, BETO remains unbeaten for POS and NER. As a further contribution, all models are publicly available to the community for future research."><link rel=alternate hreflang=en-us href=https://opencenia.github.io/models/canete-albeto-2022/><meta name=theme-color content="#1565c0"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.4567f1e7ae6323c1cc6a69d4783ef0ff.css><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hua0c6be8cf0f9f8b69c1b584196d369ff_113001_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hua0c6be8cf0f9f8b69c1b584196d369ff_113001_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://opencenia.github.io/models/canete-albeto-2022/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@wowchemy"><meta property="twitter:creator" content="@wowchemy"><meta property="og:site_name" content="OpenCENIA"><meta property="og:url" content="https://opencenia.github.io/models/canete-albeto-2022/"><meta property="og:title" content="ALBETO and DistilBETO: Lightweight Spanish Language Models | OpenCENIA"><meta property="og:description" content="In recent years there have been considerable advances in pre-trained language models, where non-English language versions have also been made available. Due to their increasing use, many lightweight versions of these models (with reduced parameters) have also been released to speed up training and inference times. However, versions of these lighter models (e.g., ALBERT, DistilBERT) for languages other than English are still scarce. In this paper we present ALBETO and DistilBETO, which are versions of ALBERT and DistilBERT pre-trained exclusively on Spanish corpora. We train several versions of ALBETO ranging from 5M to 223M parameters and one of DistilBETO with 67M parameters. We evaluate our models in the GLUES benchmark that includes various natural language understanding tasks in Spanish. The results show that our lightweight models achieve competitive results to those of BETO (Spanish-BERT) despite having fewer parameters. More specifically, our larger ALBETO model outperforms all other models on the MLDoc, PAWS-X, XNLI, MLQA, SQAC and XQuAD datasets. However, BETO remains unbeaten for POS and NER. As a further contribution, all models are publicly available to the community for future research."><meta property="og:image" content="https://opencenia.github.io/media/logo_huadb352de640ebaad14838e8a393df198_92131_300x300_fit_lanczos_3.png"><meta property="twitter:image" content="https://opencenia.github.io/media/logo_huadb352de640ebaad14838e8a393df198_92131_300x300_fit_lanczos_3.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-04-18T22:08:23+00:00"><meta property="article:modified_time" content="2022-01-01T00:00:00+00:00"><title>ALBETO and DistilBETO: Lightweight Spanish Language Models | OpenCENIA</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=2a935df78679e245e386431093af2393><script src=/js/wowchemy-init.min.14a0ed61c6dbd594b9c75193b25be179.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/><img src=/media/logo_huadb352de640ebaad14838e8a393df198_92131_0x70_resize_lanczos_3.png alt=OpenCENIA></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/><img src=/media/logo_huadb352de640ebaad14838e8a393df198_92131_0x70_resize_lanczos_3.png alt=OpenCENIA></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/software><span>Software</span></a></li><li class=nav-item><a class=nav-link href=/models><span>Models</span></a></li><li class=nav-item><a class=nav-link href=/datasets><span>Datasets</span></a></li><li class=nav-item><a class=nav-link href=/shared_tasks><span>Shared Tasks</span></a></li><li class=nav-item><a class=nav-link href=/benchmarks><span>Benchmarks</span></a></li><li class=nav-item><a class=nav-link href=/courses><span>Courses</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>ALBETO and DistilBETO: Lightweight Spanish Language Models</h1><div class=article-metadata><div><span><a href=/author/jose-canete/>José Cañete</a></span>, <span><a href=/author/sebastian-donoso/>Sebastián Donoso</a></span>, <span><a href=/author/felipe-bravo-marquez/>Felipe Bravo-Marquez</a></span>, <span><a href=/author/andres-carvallo/>Andrés Carvallo</a></span>, <span><a href=/author/vladimir-araujo/>Vladimir Araujo</a></span></div><span class=article-date>Jan 1, 2022</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=https://arxiv.org/abs/2204.09145 target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary btn-page-header" href=https://github.com/OpenCENIA/lightweight-spanish-language-models target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary btn-page-header" href=https://huggingface.co/CenIA target=_blank rel=noopener>Models</a>
<a class="btn btn-outline-primary btn-page-header" href=https://arxiv.org/abs/2204.09145 target=_blank rel=noopener>arXiv</a></div></div><div class=article-container><div class=article-style><p>In recent years there have been considerable advances in pre-trained language models, where non-English language versions have also been made available. Due to their increasing use, many lightweight versions of these models (with reduced parameters) have also been released to speed up training and inference times. However, versions of these lighter models (e.g., ALBERT, DistilBERT) for languages other than English are still scarce. In this paper we present ALBETO and DistilBETO, which are versions of ALBERT and DistilBERT pre-trained exclusively on Spanish corpora. We train several versions of ALBETO ranging from 5M to 223M parameters and one of DistilBETO with 67M parameters. We evaluate our models in the GLUES benchmark that includes various natural language understanding tasks in Spanish. The results show that our lightweight models achieve competitive results to those of BETO (Spanish-BERT) despite having fewer parameters. More specifically, our larger ALBETO model outperforms all other models on the MLDoc, PAWS-X, XNLI, MLQA, SQAC and XQuAD datasets. However, BETO remains unbeaten for POS and NER. As a further contribution, all models are publicly available to the community for future research.</p></div><div class=article-tags><a class="badge badge-light" href=/tag/albeto/>ALBETO</a>
<a class="badge badge-light" href=/tag/distilbeto/>DistilBETO</a>
<a class="badge badge-light" href=/tag/beto/>BETO</a>
<a class="badge badge-light" href=/tag/bert/>BERT</a>
<a class="badge badge-light" href=/tag/language-model/>language model</a>
<a class="badge badge-light" href=/tag/spanish/>spanish</a>
<a class="badge badge-light" href=/tag/lightweight/>lightweight</a></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://opencenia.github.io/models/canete-albeto-2022/&text=ALBETO%20and%20DistilBETO:%20Lightweight%20Spanish%20Language%20Models" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://opencenia.github.io/models/canete-albeto-2022/&t=ALBETO%20and%20DistilBETO:%20Lightweight%20Spanish%20Language%20Models" target=_blank rel=noopener class=share-btn-facebook aria-label=facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=ALBETO%20and%20DistilBETO:%20Lightweight%20Spanish%20Language%20Models&body=https://opencenia.github.io/models/canete-albeto-2022/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://opencenia.github.io/models/canete-albeto-2022/&title=ALBETO%20and%20DistilBETO:%20Lightweight%20Spanish%20Language%20Models" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=ALBETO%20and%20DistilBETO:%20Lightweight%20Spanish%20Language%20Models%20https://opencenia.github.io/models/canete-albeto-2022/" target=_blank rel=noopener class=share-btn-whatsapp aria-label=whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://opencenia.github.io/models/canete-albeto-2022/&title=ALBETO%20and%20DistilBETO:%20Lightweight%20Spanish%20Language%20Models" target=_blank rel=noopener class=share-btn-weibo aria-label=weibo><i class="fab fa-weibo"></i></a></li></ul></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2022 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.0047d4febf356e7f0b988e541f50b065.js></script>
<script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/en/js/wowchemy.min.51d2736755dc548e60c4649aaa3eeceb.js></script>
<script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.b0d291ed6d27eacec233e6cf5204f99a.js type=module></script></body></html>